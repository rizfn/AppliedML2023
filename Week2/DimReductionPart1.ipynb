{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction 1\n",
    "\n",
    "__Week 2 - 4 May 2022__\n",
    "\n",
    "Practice applying and using dimensionality rediuction for analysing datasets with Principal Component Analysis (`PCA` & `Kernel PCA`).\n",
    "\n",
    "---\n",
    "### Data\n",
    "\n",
    "The dataset is a photometric catalogue of galaxies. These galaxies were found in the 2-square degree field on the sky called COSMOS by space- and ground-based telescopes.\n",
    "\n",
    "The radiation flux (energy per second) of each galaxy is measured in 8 bands (i.e. wavelengths of light) that span the spectrum from <span style=\"color:blue;\">blue</span> to <span style=\"color:rgb(192,4,1,1);\">infrared</span>: `u, r, z++, yHSC, H, Ks, SPLASH1, SPLASH2`. The fluxes are not corrected for any effects, such as distance to a galaxy, therefore there is a systematic effect in their measurements (called redshift).\n",
    "\n",
    "So, in addition to its photometry each galaxy has its observed bias and physical properties:\n",
    "* `redshift`$^1$ - systematic bias in flux measurements.\n",
    "* `log_mass` - stellar mass in units of $log_{10}$ (inferred from a combination of fluxes and redshifts).\n",
    "* `log_sfr` - rate of star formation in units of $log_{10}$ (inferred from a combination of fluxes and redshifts).\n",
    "* `is_star_forming` - classification, based on galaxy colours (inferred from a combinations of fluxes and redshifts).\n",
    "\n",
    "<span style=\"font-size:0.9em;\"> $^1$ - redshift is the reddening of light that is proportianal to the velocity of an object receding away. On the sky, object velocities are proportional to their distances from us ([find out more](https://www.anisotropela.dk/encyclo/redshift.html)). </span>\n",
    "\n",
    "---\n",
    "### Exercise\n",
    "\n",
    "Analyze the galaxy catalogue applying dimensionality reduction to galaxy fluxes.\n",
    "\n",
    "* Apply `PCA` to fluxes. Can you find a base of principal compoenents that separates galaxies into star forming and dead? Does PCA give you a way to differentiate between various properties of galaxies?\n",
    "* Think about preprocessing the data, if you haven't yet, and see if you can find a more representative set of principal components.\n",
    "* Apply `Kernel PCA` afterwards. Does this give you a more meaningful vector space? If so, why?\n",
    "* Apply `t-SNE`. Does it give you a cleaner separation between objects with different properties?\n",
    "* Apply `UMAP`, for comparison.\n",
    "\n",
    "---\n",
    "* Authors:  Vadim Rusakov, Charles Steinhardt\n",
    "* Email:  vadim.rusakov@nbi.ku.dk\n",
    "* Date:   27th of April 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1. Wine data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the wine data set and practice finding the underlying classes of data using simple Principle Component Analysis. \n",
    "\n",
    "What's the \"Wine data set\"? It is a clean multi-variate set used to demonstrate classification algorithms. There are 13 features:\n",
    "\n",
    "`1.  Alcohol`\\\n",
    "`2.  Malic acid`\\\n",
    "`3.  Ash`\\\n",
    "`4.  Alcalinity of ash`\\\n",
    "`5.  Magnesium`\\\n",
    "`6.  Total phenols`\\\n",
    "`7.  Flavanoids`\\\n",
    "`8.  Nonflavanoid phenols`\\\n",
    "`9.  Proanthocyanins`\\\n",
    "`10. Color intensity`\\\n",
    "`11. Hue`\\\n",
    "`12. OD280/OD315 of diluted wines`\\\n",
    "`13. Proline`\n",
    "\n",
    "The data set is small and contains three classes of objects, each with a fairly well-defined feature space. These features have largely linear relationships, which makes it well-suited for demonstrating how PCA works.\n",
    "\n",
    "Source: https://archive.ics.uci.edu/ml/datasets/wine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick function for plotting our PCA components\n",
    "def plot_pca(y_pcs, y):\n",
    "    #=== plot PCA results\n",
    "    fig, ax = plt.subplots(1, figsize=(5, 5), dpi=100)\n",
    "    #ax.set_xlim(np.percentile(y_pcs[:,0], 99), np.percentile(y_pcs[:,0], 1))\n",
    "    #ax.set_ylim(np.percentile(y_pcs[:,1], 99), np.percentile(y_pcs[:,1], 1))\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "\n",
    "    # locate the points of each type in the original data\n",
    "    # and paint them over the transformed data\n",
    "    is_type1 = (y == 0)\n",
    "    is_type2 = (y == 1)\n",
    "    is_type3 = (y == 2)\n",
    "    ax.scatter(y_pcs[is_type1, 0], y_pcs[is_type1, 1], \n",
    "               c='y', marker='s', label='Type 1')\n",
    "    ax.scatter(y_pcs[is_type2, 0], y_pcs[is_type2, 1], \n",
    "               c='b', marker='o', label='Type 2')\n",
    "    ax.scatter(y_pcs[is_type3, 0], y_pcs[is_type3, 1], \n",
    "               c='g', marker='^', label='Type 3')\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, load the data, split it into features `X` and labels `y`. Then apply PCA to it and plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the wine dataset\n",
    "data = load_wine()\n",
    "X = data['data']\n",
    "y = data['target']\n",
    "\n",
    "pca = PCA(n_components=2, svd_solver='full') # get a pca object of class PCA()\n",
    "y_pcs = pca.fit_transform(X) # train pca object\n",
    "\n",
    "# plot PCA results\n",
    "plot_pca(y_pcs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the three groups do not appear to be distinct. However, we would expect for these classes to have distinct principal components. So something does not quite work. Remember to check the distributions of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(4, 4), dpi=100)\n",
    "ax.set_xlabel('log10( Variable values )')\n",
    "ax.set_ylabel('Number')\n",
    "ax.set_title('Raw data')\n",
    "xbins = np.arange(-1, 3.5, 0.1)\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    ax.hist(np.log10(X[:, i]), bins=xbins, histtype='step', label=f'Var {i}')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see that the distributions of variables are normalized very differently. Let's fix it and try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize X\n",
    "transform = preprocessing.Normalizer(norm='l2')\n",
    "X_norm = transform.fit_transform(X)\n",
    "\n",
    "# transform X_norm\n",
    "y_pcs = pca.fit_transform(X_norm) # train pca object\n",
    "\n",
    "# plot new PCA results\n",
    "plot_pca(y_pcs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still does not work. Let's see those new distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(4, 4), dpi=100)\n",
    "ax.set_xlabel('log10( Variable values )')\n",
    "ax.set_ylabel('Number')\n",
    "ax.set_title('Normalized data')\n",
    "xbins = np.arange(-4, 0, 0.1)\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    x_pos = X_norm[:, i][X_norm[:, i] > 0.0]\n",
    "    ax.hist(np.log10(x_pos), bins=xbins, histtype='step', label=f'Var {i}')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They are centered similarly, but seem to have different scales. Can we do better by scaling them similarly in addition to normalizing (i.e., standardizing them)? Looks much better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize X\n",
    "transform = preprocessing.StandardScaler()\n",
    "X_std = transform.fit_transform(X)\n",
    "\n",
    "# transform X_std\n",
    "y_pcs = pca.fit_transform(X_std) # train pca object\n",
    "\n",
    "# plot new PCA results\n",
    "plot_pca(y_pcs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See, the distributions are much more similar now. You can play around with other data transformers too. You may want to think about whether you have any outliers and how to deal with them. Also, sometimes variables can have different underlying distributions and therefore need to be transformed to the same (ideally, symmetric and standardized) distribution. Here is the link:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(4, 4), dpi=100)\n",
    "ax.set_xlabel('log10( Variable values )')\n",
    "ax.set_ylabel('Number')\n",
    "ax.set_title('Normalized data')\n",
    "xbins = np.arange(-3.2, 1.0, 0.4)\n",
    "\n",
    "for i in range(X.shape[1]):\n",
    "    x_pos = X_std[:, i][X_std[:, i] > 0.0]\n",
    "    ax.hist(np.log10(x_pos), bins=xbins, histtype='step', label=f'Var {i}')\n",
    "\n",
    "ax.legend(loc=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can comoute the principal components using a non-linear kernel, which can slightly improve the classification in this case. \n",
    "\n",
    "`KernelPCA` is a variant of the PCA, which can use a range of kernels for non-linear operations. I.e., this extension gives flexibility in separating the data that are not linearly-separable. Make sure to try different kernels for reducing the dimensionality. See documentation for `KernelPCA` in **sklearn**.\n",
    "\n",
    "For Kernel PCA see the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize X\n",
    "transform = preprocessing.StandardScaler()\n",
    "X_std = transform.fit_transform(X)\n",
    "\n",
    "# transform X_norm\n",
    "kpca = KernelPCA(n_components=2, kernel='rbf')\n",
    "y_pcs = kpca.fit_transform(X_std)\n",
    "\n",
    "# plot new PCA results\n",
    "plot_pca(y_pcs, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2. \"Galaxies\" data set.\n",
    "\n",
    "Can you find odd samples that are mixed into the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"datasets/cosmos2015_outliers.csv\"\n",
    "df_orig = pd.read_csv(file, index_col=False)\n",
    "df_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a random sub-sample of the dataset. `PCA` does computations linearly, therefore it's quick and you can choose the whole dataset if you wish. Then take 1000 objects at random and break one of their features for them. Will our algorithsm manage to isolate these ourliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a random sub-sample of the dataset\n",
    "n = 10000\n",
    "idxs = np.arange(df_orig.shape[0])\n",
    "idxs_rand = np.random.choice(idxs, size=n)\n",
    "df = df_orig.iloc[idxs_rand] # dataframe\n",
    "\n",
    "# take our data set to work with\n",
    "X = df.values # array\n",
    "\n",
    "# get column names\n",
    "flux_cols = list(df_orig.columns[4:]) # flux column names\n",
    "flux_idxs = np.argwhere(np.isin(df.columns, flux_cols)).flatten() # flux column indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now take the galaxy data (fluxes) and find out whether you can reduce it to a couple of meaningful principal components using `PCA`. By meaningful, we are interested in the method that is capable of separating galaxies into `star forming` or `dead`.\n",
    "\n",
    "The user interface of the PCA in sklearn is the same as for all other similar classes (see PCA [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)). Complexity of PCA with full `svd` solve: $\\mathcal{O}(n_{max}^2 n_{min})$, where $n_{max} = max(n_{samples}, n_{features})$, $n_{min} = min(n_{samples}, n_{features})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize X\n",
    "# try standardizing the fluxes for PCA?\n",
    "# transform = preprocessing.StandardScaler()\n",
    "# X_pca_std = transform.fit_transform(X[:, flux_idxs])\n",
    "\n",
    "# now compute PCs\n",
    "pca = PCA(n_components=2, svd_solver='full') # get a pca object of class PCA()\n",
    "y_pcs = pca.fit_transform() # train pca object on fluxes (raw observed data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the plots below we can overplot the now known outlier data points so we can judge explicitly whether the algorithms do a good job at finding these odd data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_components(y_pcs, color_property, cmap='viridis', clabel=''):\n",
    "    # set up the figure\n",
    "    fig, ax = plt.subplots(1, figsize=(5, 5), dpi=100)\n",
    "    #ax.set_xlim(np.percentile(y_pcs[:,0], 99), np.percentile(y_pcs[:,0], 1))\n",
    "    #ax.set_ylim(np.percentile(y_pcs[:,1], 99), np.percentile(y_pcs[:,1], 1))\n",
    "    ax.set_xlabel('Component 1')\n",
    "    ax.set_ylabel('Component 2')\n",
    "    \n",
    "    # choose color property\n",
    "    y_color = df_cut[color_property]\n",
    "    vmin, vmax = np.percentile(y_color, 99), np.percentile(y_color, 1)\n",
    "    \n",
    "    # plot galaxies\n",
    "    sc = ax.scatter(y_pcs[:, 0], y_pcs[:, 1], s=0.02, \n",
    "                    c=y_color, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    ax.scatter(y_pcs[mask_sample, 0], y_pcs[mask_sample, 1], s=0.2, c='k')\n",
    "    \n",
    "    # add colorbar\n",
    "    cbar = plt.colorbar(sc)\n",
    "    cbar.ax.set_ylabel(clabel, rotation=270, labelpad=10)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "plot_components(y_pcs, color_property='is_star_forming', \n",
    "                cmap='bwr', clabel='Star-forming (=1) / Dead (=0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, in the above dimensions we cannot draw a **decision boundary** to separate the living galaxies from the dead. Therefore, the classification is not physically meaningful to us. But out of curiosity, let us check the other physical properties, such as the galaxy mass `log_mass` and the rate of star formation `log_sfr`, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_pcs, color_property='log_mass', cmap='jet', clabel='Mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_pcs, color_property='log_sfr', \n",
    "                  cmap='jet', clabel='Star Formation Rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, all objects are mixed together in not a helpful way... What about the last property, the `redshift`s? This feature is supposed to introduce a bias in the fluxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_pcs, color_property='redshift', \n",
    "                  cmap='jet', clabel='Redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that separation in `redshift` is better than for other properties. Therefore, the conclusion here is not that `PCA` is a particularly bad choice, but rather that it picks up the strongest signal affecting the data, which in this case turns out to be a systematic effect of the redshift. However, the data here has a continous (NOT discrete) distribution of redshift and therefore we should not expect to find separate clusters of data points. \n",
    "\n",
    "So, if we would like to reduce dimensions meaningfully in other properties of interest, we need to correct for the effect of redshift first. You will attempt to do this in the next session, when you will pay more attention to preprocessing and selection of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let us continue throwing these data at other algorithms to get some practice with them - `KernelPCA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try the same data with KernalPCA now\n",
    "kpca = KernelPCA(n_components=2, kernel='sigmoid')\n",
    "y_pcs = kpca.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_pcs, color_property='is_star_forming', \n",
    "                  cmap='bwr', clabel='Star-forming (=1) / Dead (=0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, the projections above are better than with the linear `PCA`, but still quite mixed. Below, you can see that the redshifts an apparent signal picked up by the non-linear PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_pcs, color_property='redshift', \n",
    "                  cmap='jet', clabel='Redshift')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try to run `t-SNE` on the dataset (for examples or set-up see documentation for `t-SNE` on sklearn [website](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)). Use `perplexity=50, method='barnes_hut', n_iter=1000, random_state=42` for now. In the next class we will put more emphasis on the importance of the optimal values for theses parameters. Complexity of `t-SNE` (with approximate solver `barnes_hut`) is $\\mathcal{O}(k n^2)$, where $k$ - number of output dimensions, $n$ - number of samples.\n",
    "\n",
    "* How well does `t-SNE` help to differentiate between two classes here?\n",
    "\n",
    "* Does you get clusters of galaxies or a continuum?\n",
    "\n",
    "* Which physical property is the most distinctly separated in the reduced space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# running t-SNE\n",
    "# use the non-standardized data X[:, flux_idxs]\n",
    "tsne = TSNE()\n",
    "y_tsne = tsne.fit_transform()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_tsne, color_property='is_star_forming', \n",
    "                  cmap='bwr', clabel='Star-forming (=1) / Dead (=0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the map produced with `t-SNE` is much more meaningful to us. It is not ideal and there are plenty of stray objects in either red or blue group, but we can even compute a decision boundary for these projections to do probably quite accurate classification. \n",
    "\n",
    "The black outliers come out very explicitly on the t-SNE mapping in s 'snake' pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_tsne, color_property='redshift', \n",
    "                  cmap='jet', clabel='Redshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_tsne, color_property='log_mass', \n",
    "                  cmap='jet', clabel='Mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_tsne, color_property='log_sfr', \n",
    "                  cmap='jet', clabel='Star formation rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UMAP\n",
    "\n",
    "Now try using `UMAP`. For documentation see the UMAP [webpage](https://umap-learn.readthedocs.io/en/latest/api.html). This has the same interface as the other embedding classes above. Use with `n_components=2, n_neighbors=50, random_state=42`. \n",
    "\n",
    "* Do you get something similar to `t-SNE`?\n",
    "\n",
    "* How well can you map different properties in the reduced space?\n",
    "\n",
    "* Do you get clusters or continuous distributions? Which physical property is the most strongly separable with `UMAP`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "# running UMAP\n",
    "# use the non-standardized data X[:, flux_idxs]\n",
    "map_obj = umap.UMAP()\n",
    "y_umap = map_obj.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This probably is best at finding the outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_umap, color_property='is_star_forming', \n",
    "                  cmap='jet', clabel='Star-forming (=1) / Dead (=0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_umap, color_property='redshift', \n",
    "                  cmap='jet', clabel='Redshift')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_umap, color_property='log_mass', \n",
    "                  cmap='jet', clabel='Mass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components(y_umap, color_property='log_sfr', \n",
    "                  cmap='jet', clabel='Star formation rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are interested in seeing how different dimension reduction algorithms' complexities compare, see the plots here:\n",
    "\n",
    "https://umap-learn.readthedocs.io/en/latest/benchmarking.html"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5875523c0f295e9a7a392db41a9a428ae2dda35a82a766a28c29c1f8950fbf4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
