{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of b-quark jets in the Aleph simulated data\n",
    "\n",
    "Python macro for selecting b-jets (sprays of particles with a b-quark in it) in Aleph Z -> qqbar MC (i.e. simulated decays of the Z0 boson decaying to a quark and an anti-quark) in various ways:\n",
    "* Initially, simply with \"if\"-statements making requirements on certain variables. This corresponds to selecting \"boxes\" in the input variable space (typically called \"X\"). One could also try a Fisher discriminant (linear combination of input variables), which corresponds to a plane in the X-space. But as the problem is non-linear, it is likely to be sub-optimal.\n",
    "\n",
    "* Next using Machine Learning (ML) methods. We will during the first week try both (Boosted) Decision Tree ((B)DT) based and Neural Net (NN) based methods, and see how complicated (or not) it is to get a good solution, and how much better it performs compared to the \"classic\" selection method.\n",
    "\n",
    "Once you obtain a classification of b-jets vs. non-b-jets, think about how to quantify the quality of your algorithm. Also, try to compare it to the NN result of the Aleph collaboration, given by the variable \"nnbjet\". It is based on a neural network with 6 input variables (prob_b, spheri, pt2rel, multip, bqvjet, and ptlrel), and two hidden layers each with 10 nodes in. Can you do better?\n",
    "\n",
    "In the end, this exercise is the simple start on moving into the territory of Machine Learning analysis.\n",
    "\n",
    "\n",
    "### Data:\n",
    "The input variables (X) are (where Aleph uses only the first six):\n",
    "* **prob_b**: Probability of being a b-jet from the pointing of the tracks to the vertex.\n",
    "* **spheri**: Sphericity of the event, i.e. how spherical it is.\n",
    "* **pt2rel**: The transverse momentum squared of the tracks relative to the jet axis, i.e. width of the jet.\n",
    "* **multip**: Multiplicity of the jet (in a relative measure).\n",
    "* **bqvjet**: b-quark vertex of the jet, i.e. the probability of a detached vertex.\n",
    "* **ptlrel**: Transverse momentum (in GeV) of possible lepton with respect to jet axis (about 0 if no leptons).\n",
    "* energy: Measured energy of the jet in GeV. Should be 45 GeV, but fluctuates.\n",
    "* cTheta: cos(theta), i.e. the polar angle of the jet with respect to the beam axis. Note, that the detector works best in the central region (|cTheta| small) and less well in the forward regions.\n",
    "* phi:    The azimuth angle of the jet. As the detector is uniform in phi, this should not matter (much).\n",
    "\n",
    "The target variable (Y) is:\n",
    "* isb:    1 if it is from a b-quark and 0, if it is not.\n",
    "\n",
    "Finally, those before you (the Aleph collaboration in the mid 90'ies) produced a Neural Net (6 input variables, two hidden layers with 10 neurons in each, and 1 output varible) based classification variable, which you can compare to (and compete with?):\n",
    "* nnbjet: Value of original Aleph b-jet tagging algorithm, using only the last six variables (for reference).\n",
    "\n",
    "\n",
    "### Task:\n",
    "Thus, the task before you is to produce functions (non-ML and then ML algorithm), which given the input variables X provides an output variable estimate, Y_est, which is \"closest possible\" to the target variable, Y. The \"closest possible\" is left to the user to define in a _Loss Function_, which we will discuss further. In classification problems (such as this), the typical loss function to use \"Cross Entropy\", see https://en.wikipedia.org/wiki/Cross_entropy.\n",
    "\n",
    "Once you have results, you're welcome to continue with a Fisher Linear Discriminant, and you may also challenge yourself by considering \"v1\" of the data, which is a little less \"polished and ready\". If you also manage this, then don't hold back in applying a real ML algorithm to the problem (you can get inspiration from \"ML_MethodsDemos.ipynb\" or the vast internet). A suggestion might be XGBoost or LightGBM.\n",
    "\n",
    "* Author: Troels C. Petersen (NBI)\n",
    "* Email:  petersen@nbi.dk\n",
    "* Date:   17th of April 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division   # Ensures Python3 printing & division standard\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import LogNorm\n",
    "import numpy as np\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Possible other packages to consider:\n",
    "cornerplot, seaplot, sklearn.decomposition(PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random\n",
    "r.seed(42)\n",
    "\n",
    "SavePlots = False\n",
    "plt.close('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate an attempt at classification:\n",
    "\n",
    "This is made into a function, as this is called many times. It returns a \"confusion matrix\" and the fraction of wrong classifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(bquark) :\n",
    "    N = [[0,0], [0,0]]   # Make a list of lists (i.e. confusion matrix) for counting successes/failures.\n",
    "    for i in np.arange(len(isb)):\n",
    "        if (bquark[i] == 0 and isb[i] == 0) : N[0][0] += 1\n",
    "        if (bquark[i] == 0 and isb[i] == 1) : N[0][1] += 1\n",
    "        if (bquark[i] == 1 and isb[i] == 0) : N[1][0] += 1\n",
    "        if (bquark[i] == 1 and isb[i] == 1) : N[1][1] += 1\n",
    "    fracWrong = float(N[0][1]+N[1][0])/float(len(isb))\n",
    "    accuracy = 1.0 - fracWrong\n",
    "    return N, accuracy, fracWrong"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main program start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data (with this very useful NumPy reader):\n",
    "data = np.genfromtxt('AlephBtag_MC_train_Nev5000.csv', names=True)    # For faster running\n",
    "# data = np.genfromtxt('AlephBtag_MC_train_Nev50000.csv', names=True)   # For more data\n",
    "\n",
    "# Kinematics (energy and direction) of the jet:\n",
    "energy = data['energy']\n",
    "cTheta = data['cTheta']\n",
    "phi    = data['phi']\n",
    "\n",
    "# Classification variables (those used in Aleph's NN):\n",
    "prob_b = data['prob_b']\n",
    "spheri = data['spheri']\n",
    "pt2rel = data['pt2rel']\n",
    "multip = data['multip']\n",
    "bqvjet = data['bqvjet']\n",
    "ptlrel = data['ptlrel']\n",
    "\n",
    "# Aleph's NN score:\n",
    "nnbjet = data['nnbjet']\n",
    "\n",
    "# Truth variable whether it really was a b-jet or not (i.e. target)\n",
    "isb    = data['isb']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect the data:\n",
    "Define the histogram range and binning (important - programs are generally NOT good at this):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nbins = 100\n",
    "xmin = 0.0\n",
    "xmax = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make new lists selected based on what the jets really are (b-quark jet or light-quark jet):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_b_bjets = prob_b[isb == 1]\n",
    "prob_b_ljets = prob_b[isb == 0]\n",
    "bqvjet_bjets = bqvjet[isb == 1]\n",
    "bqvjet_ljets = bqvjet[isb == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce 1D figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the actual figure, here with two histograms in it:\n",
    "fig, ax = plt.subplots(figsize=(12, 6))      # Create just a single figure and axes (figsize is in inches!)\n",
    "hist_prob_b_bjets = ax.hist(prob_b_bjets, bins=Nbins, range=(xmin, xmax), histtype='step', linewidth=2, label='prob_b_bjets', color='blue')\n",
    "hist_prob_b_ljets = ax.hist(prob_b_ljets, bins=Nbins, range=(xmin, xmax), histtype='step', linewidth=2, label='prob_b_ljets', color='red')\n",
    "ax.set_xlabel(\"Probability of b-quark based on track impact parameters\")     # Label of x-axis\n",
    "ax.set_ylabel(\"Frequency / 0.01\")                                            # Label of y-axis\n",
    "ax.set_title(\"Distribution of prob_b\")                                       # Title of plot\n",
    "ax.set_yscale(\"log\")\n",
    "ax.legend(loc='best')                                                        # Legend. Could also be 'upper right'\n",
    "ax.grid(axis='y')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.show()\n",
    "\n",
    "if SavePlots :\n",
    "    fig.savefig('Hist_prob_b_and_bqvjet.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Produce 2D figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we try a scatter plot, to see how the individual events distribute themselves:\n",
    "fig2, ax2 = plt.subplots(figsize=(12, 6))\n",
    "scat2_prob_b_vs_bqvjet_bjets = ax2.scatter(prob_b_bjets, bqvjet_bjets, label='b-jets', color='blue')\n",
    "scat2_prob_b_vs_bqvjet_ljets = ax2.scatter(prob_b_ljets, bqvjet_ljets, label='l-jets', color='red')\n",
    "ax2.legend(loc='best')\n",
    "fig2.tight_layout()\n",
    "fig2.show()\n",
    "\n",
    "if SavePlots :\n",
    "    fig2.savefig('Scatter_prob_b_vs_bqvjet.pdf', dpi=600)\n",
    "\n",
    "\n",
    "# However, as can be seen in the figure, the overlap between b-jets and light-jets is large,\n",
    "# and one covers much of the other in a scatter plot, which also does not show the amount of\n",
    "# statistics in the dense regions. Therefore, we try two separate 2D histograms (zoomed):\n",
    "fig3, ax3 = plt.subplots(1, 2, figsize=(12, 6))\n",
    "hist2_prob_b_vs_bqvjet_bjets = ax3[0].hist2d(prob_b_bjets, bqvjet_bjets, bins=[40,40], range=[[0.0, 0.4], [0.0, 0.4]])\n",
    "hist2_prob_b_vs_bqvjet_ljets = ax3[1].hist2d(prob_b_ljets, bqvjet_ljets, bins=[40,40], range=[[0.0, 0.4], [0.0, 0.4]])\n",
    "ax3[0].set_title(\"b-jets\")\n",
    "ax3[1].set_title(\"light-jets\")\n",
    "\n",
    "fig3.tight_layout()\n",
    "fig3.show()\n",
    "\n",
    "if SavePlots :\n",
    "    fig3.savefig('Hist2D_prob_b_vs_bqvjet.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I give the selection cuts names, so that they only need to be changed in ONE place (also ensures consistency!):\n",
    "# Note: This is where you change things (selection values and adding variables) to improve the performance:\n",
    "cut_bqvjet = 0.20\n",
    "\n",
    "# Think about how the above cuts \"works\", and then imagine what I have in mind below, trying to refine the selection:\n",
    "# cut_propb = 0.15\n",
    "# loose_propb = 0.07\n",
    "# tight_propb = 0.26\n",
    "# loose_bqvjet = 0.09\n",
    "# tight_bqvjet = 0.34\n",
    "\n",
    "# If prob_b indicate b-quark, call it a b-quark, otherwise not!\n",
    "bquark=[]\n",
    "for i in np.arange(len(prob_b)):\n",
    "    if (bqvjet[i] > cut_bqvjet) :\n",
    "        bquark.append(1)\n",
    "    else :\n",
    "        bquark.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, accuracy, fracWrong = evaluate(bquark)\n",
    "print(\"\\nRESULT OF HUMAN ATTEMPT AT A GOOD SELECTION:\")\n",
    "print(\"  First number in parenthesis is the estimate, second is the MC truth:\")\n",
    "print(\"  True-Negative (0,0)  = \", N[0][0])\n",
    "print(\"  False-Negative (0,1) = \", N[0][1])\n",
    "print(\"  False-Positive (1,0) = \", N[1][0])\n",
    "print(\"  True-Positive (1,1)  = \", N[1][1])\n",
    "print(\"    Fraction wrong            = ( (0,1) + (1,0) ) / sum = \", fracWrong)\n",
    "print(\"    Fraction right (accuracy) = ( (0,0) + (1,1) ) / sum = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with NN-approach from 1990'ies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bquark=[]\n",
    "for i in np.arange(len(prob_b)):\n",
    "    if   (nnbjet[i] > 0.82) : bquark.append(1)\n",
    "    else : bquark.append(0)\n",
    "\n",
    "N, accuracy, fracWrong = evaluate(bquark)\n",
    "print(\"\\nALEPH BJET TAG:\")\n",
    "print(\"  First number in parenthesis is the estimate, second is the MC truth:\")\n",
    "print(\"  True-Negative (0,0)  = \", N[0][0])\n",
    "print(\"  False-Negative (0,1) = \", N[0][1])\n",
    "print(\"  False-Positive (1,0) = \", N[1][0])\n",
    "print(\"  True-Positive (1,1)  = \", N[1][1])\n",
    "print(\"    Fraction wrong            = ( (0,1) + (1,0) ) / sum = \", fracWrong)\n",
    "print(\"    Fraction right (accuracy) = ( (0,0) + (1,1) ) / sum = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Suggested problems:\n",
    "\n",
    "1. Start by plotting the six \"Aleph classification variables\" for signal and background, and see which seems to separate most. Possibly draw ROC curves for all of these separately, to quantify this.\n",
    "\n",
    "2. As you can see, the Aleph-NN performs significantly better. Try to optimize your cuts, combining several of them in smart ways. You may also want to make a linear combination (i.e. Fisher Linear Discriminant) to do better. Challenge yourself to push it as far as you can (well, for say 30 minutes).\n",
    "\n",
    "3. Does including more data (50000 instead of 5000 events) help your performance?\n",
    "\n",
    "4. Currently, the scoring (also called the loss function) is simply done by considering the fraction of wrong estimates. Think about what the alternatives could be, especially if you were to give a continuous score like the Aleph-NN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning points:\n",
    "\n",
    "From this exercise you should get a feel for the problem at hand, namely how to separate two populations in a 6-dimensional space. It is hard to imagine, yet with simple cuts you should be able to get \"some performance\", though never anywhere close to the Aleph-NN. You should learn, that it is hard, but that at least the fact that you have known cases makes you capable of getting that \"some performance\". And you should of course be able to draw ROC curves to compare performances.\n",
    "\n",
    "The next steps (i.e. following exercises) are to improve this performance through the use of Machine Learning (ML), and make you capable not only of getting results, but also confident in optimising them, and certainly proficient in interpreting them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example solution(s):\n",
    "\n",
    "The first thing to do is to plot the data, and also look into how correlated it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6x6 plot of the variable distributions (diagonal) and their mutual correlations (off-diagonal):\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data)                                        # Make the data into a DataFrame\n",
    "df = df.drop(['energy','cTheta','phi','nnbjet'], axis=1)       # Drop the irrelevant variables\n",
    "\n",
    "import seaborn as sns\n",
    "sns.pairplot(df, hue=\"isb\", diag_kind='hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the variable correlation (here Pearson, i.e. linear):\n",
    "\n",
    "# First print the correlations for ALL events:\n",
    "print(df.corr(method='pearson'))\n",
    "\n",
    "# Divide data into b-jets and non-b-jets:\n",
    "df_b = df.loc[df['isb'] == 1].drop(['isb'], axis=1)\n",
    "df_l = df.loc[df['isb'] == 0].drop(['isb'], axis=1)\n",
    "\n",
    "# Make two separate correlation plots, also to see differences:\n",
    "fig, ax = plt.subplots(1, 2, figsize=(18, 8))\n",
    "sns.heatmap(df_b.corr(), mask=np.zeros_like(df_b.corr(), dtype=np.bool),\n",
    "            cmap=sns.diverging_palette(220, 10, as_cmap=True), vmin=-1.0, vmax=1.0,\n",
    "            square=True, ax=ax[0])\n",
    "ax[0].set_title('b-jets')\n",
    "\n",
    "sns.heatmap(df_l.corr(), mask=np.zeros_like(df_l.corr(), dtype=np.bool),\n",
    "            cmap=sns.diverging_palette(220, 10, as_cmap=True), vmin=-1.0, vmax=1.0,\n",
    "            square=True, ax=ax[1])\n",
    "ax[1].set_title('non b-jets');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments on plots:\n",
    "\n",
    "As can be seen, the variables correlate similarly between the two types of jets. In particular 'prob_b' and 'bqvjet' are very correlated, which is the reason why performance doesn't increase a lot, when both of these powerful variables are included: They hold similar power, which can only be used once!\n",
    "\n",
    "The variable 'ptlrel' is not very correlated to any other variables, which means that it contributes no matter which other variables are included. Of course in the end, one wants to include all variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Solution 1 (simply choose three variables):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here starts the example solution!\n",
    "   \n",
    "# I give the selection cuts names, so that they only need to be changed in ONE place (also ensures consistency!):\n",
    "loose_propb = 0.10\n",
    "tight_propb = 0.16\n",
    "loose_bqvjet = 0.12\n",
    "tight_bqvjet = 0.28\n",
    "loose_ptlrel = 0.40\n",
    "tight_ptlrel = 0.60\n",
    "\n",
    "bquark=[]\n",
    "for i in np.arange(len(prob_b)):\n",
    "    if   (prob_b[i] > tight_propb)  : bquark.append(1)\n",
    "    elif (bqvjet[i] > tight_bqvjet) : bquark.append(1)\n",
    "    elif (ptlrel[i] > tight_ptlrel) : bquark.append(1)\n",
    "    elif ((prob_b[i] > loose_propb)  and (bqvjet[i] > loose_bqvjet)) : bquark.append(1)\n",
    "    elif ((prob_b[i] > loose_propb)  and (ptlrel[i] > loose_ptlrel)) : bquark.append(1)\n",
    "    elif ((bqvjet[i] > loose_bqvjet) and (ptlrel[i] > loose_ptlrel)) : bquark.append(1)\n",
    "    elif ((ptlrel[i] > 0.3) and (prob_b[i] > 0.08) and (bqvjet[i] > 0.10)) : bquark.append(1)  # Adds very little!\n",
    "    else : bquark.append(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, accuracy, fracWrong = evaluate(bquark)\n",
    "print(\"\\nRESULT OF HUMAN ATTEMPT AT A GOOD SELECTION:\")\n",
    "print(\"  First number in parenthesis is the estimate, second is the MC truth:\")\n",
    "print(\"  True-Negative (0,0)  = \", N[0][0])\n",
    "print(\"  False-Negative (0,1) = \", N[0][1])\n",
    "print(\"  False-Positive (1,0) = \", N[1][0])\n",
    "print(\"  True-Positive (1,1)  = \", N[1][1])\n",
    "print(\"    Fraction wrong            = ( (0,1) + (1,0) ) / sum = \", fracWrong)\n",
    "print(\"    Fraction right (accuracy) = ( (0,0) + (1,1) ) / sum = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare with NN-approach from 1990'ies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bquark=[]\n",
    "for i in np.arange(len(prob_b)):\n",
    "    if   (nnbjet[i] > 0.82) : bquark.append(1)\n",
    "    else : bquark.append(0)\n",
    "\n",
    "N, accuracy, fracWrong = evaluate(bquark)\n",
    "print(\"\\nALEPH BJET TAG:\")\n",
    "print(\"  First number in parenthesis is the estimate, second is the MC truth:\")\n",
    "print(\"  True-Negative (0,0)  = \", N[0][0])\n",
    "print(\"  False-Negative (0,1) = \", N[0][1])\n",
    "print(\"  False-Positive (1,0) = \", N[1][0])\n",
    "print(\"  True-Positive (1,1)  = \", N[1][1])\n",
    "print(\"    Fraction wrong            = ( (0,1) + (1,0) ) / sum = \", fracWrong)\n",
    "print(\"    Fraction right (accuracy) = ( (0,0) + (1,1) ) / sum = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on Example Solution 1:\n",
    "\n",
    "I simply chose to simplify the problem by choosing only three variables and then making an \"if-elif-else\" selection with these, cutting out the regions with most b-quarks in them, and leaving the rest as non-b-quarks. Some of these regions contribute very little.\n",
    "\n",
    "The choice of three variables is also not simple, because variables may look good, but if they are very correlated with other variables (like \"prob_b\" and \"bqvjet\"), then they might not contribute that much in addition. This was also the reason for choosing \"ptlrel\" as the third variable.\n",
    "\n",
    "The solution gets a wrong fraction of <b>0.112</b>, which is reasonably good, though not close to the Aleph NN solution of <b>0.0988</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Example 2 (brute force looping over selection values to get \"optimal\" selection):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here I make a range of values (min, max) from which I choose (randomly - more on that) from:\n",
    "range_loose_prob_b = [0.08, 0.11]\n",
    "range_tight_prob_b = [0.16, 0.21]\n",
    "range_loose_bqvjet = [0.13, 0.18]\n",
    "range_tight_bqvjet = [0.28, 0.40]\n",
    "range_loose_ptlrel = [0.20, 0.40]\n",
    "range_tight_ptlrel = [0.45, 0.65]\n",
    "\n",
    "# The current best (i.e. lowest) fraction wrong. Set high initially, and lowered by better selections:\n",
    "fracWrong_min = 0.999\n",
    "\n",
    "# Try several times (here 100):\n",
    "for Ntest in range(100) :\n",
    "\n",
    "    # Choose random selection and test it:\n",
    "    loose_prob_b = r.uniform(range_loose_prob_b[0], range_loose_prob_b[1])\n",
    "    tight_prob_b = r.uniform(range_tight_prob_b[0], range_tight_prob_b[1])\n",
    "    loose_bqvjet = r.uniform(range_loose_bqvjet[0], range_loose_bqvjet[1])\n",
    "    tight_bqvjet = r.uniform(range_tight_bqvjet[0], range_tight_bqvjet[1])\n",
    "    loose_ptlrel = r.uniform(range_loose_ptlrel[0], range_loose_ptlrel[1])\n",
    "    tight_ptlrel = r.uniform(range_tight_ptlrel[0], range_tight_ptlrel[1])\n",
    "\n",
    "    bquark=[]\n",
    "    for i in np.arange(len(prob_b)):\n",
    "        if   (prob_b[i] > tight_prob_b) : bquark.append(1)\n",
    "        elif (bqvjet[i] > tight_bqvjet) : bquark.append(1)\n",
    "        elif (ptlrel[i] > tight_ptlrel) : bquark.append(1)\n",
    "        elif ((prob_b[i] > loose_prob_b) and (bqvjet[i] > loose_bqvjet)) : bquark.append(1)\n",
    "        elif ((prob_b[i] > loose_prob_b) and (ptlrel[i] > loose_ptlrel)) : bquark.append(1)\n",
    "        elif ((bqvjet[i] > loose_bqvjet) and (ptlrel[i] > loose_ptlrel)) : bquark.append(1)\n",
    "        else : bquark.append(0)\n",
    "\n",
    "    N, accuracy, fracWrong = evaluate(bquark)\n",
    "\n",
    "    # Flag/print best selection(s):\n",
    "    if (fracWrong < fracWrong_min) :\n",
    "        print(\"\\nRESULT OF TESTING MANY SELECTION CUTS:\")\n",
    "        print(\"  First number is my estimate, second is the MC truth:\", Ntest)\n",
    "        print(\"  True-Negative (0,0)  = \", N[0][0])\n",
    "        print(\"  False-Negative (0,1) = \", N[0][1])\n",
    "        print(\"  False-Positive (1,0) = \", N[1][0])\n",
    "        print(\"  True-Positive (1,1)  = \", N[1][1])\n",
    "        print(\"    Fraction wrong            = ( (0,1) + (1,0) ) / sum = \", fracWrong)\n",
    "        print(\"    Fraction right (accuracy) = ( (0,0) + (1,1) ) / sum = \", accuracy)           \n",
    "\n",
    "        fracWrong_min = fracWrong\n",
    "        print(\"  GOT A NEW MINIMUM.... GREAT!\")\n",
    "        print(\"  loose_prob_b = \", loose_prob_b)\n",
    "        print(\"  tight_prob_b = \", tight_prob_b)\n",
    "        print(\"  loose_bqvjet = \", loose_bqvjet)\n",
    "        print(\"  tight_bqvjet = \", tight_bqvjet)\n",
    "        print(\"  loose_ptlrel = \", loose_ptlrel)\n",
    "        print(\"  tight_ptlrel = \", tight_ptlrel)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on example solution 2:\n",
    "\n",
    "This solution simply expands on the first solution by searching for better parameters of the first solution. There are six parameters, and one can either do nested loops over all six, or... choose points at random! We'll discuss these approaches more.\n",
    "\n",
    "This solution improves the wrong fraction from <b>0.112</b> to <b>0.1078</b>, which is marginally better, though still not close to the Aleph NN solution of <b>0.0988</b>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example solution 3 (Decision Tree):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, plot_tree,export_graphviz\n",
    "import pandas as pd\n",
    "import graphviz\n",
    "from IPython.display import SVG\n",
    "from graphviz import Source\n",
    "from sklearn import tree\n",
    "from ipywidgets import interactive\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_xx = np.column_stack((prob_b,spheri,pt2rel,multip,bqvjet,ptlrel))\n",
    "data_yy = isb\n",
    "X = pd.DataFrame(data_xx, columns=['prob_b', 'spheri','pt2rel','multip','bqvjet','ptlrel'])\n",
    "y = pd.Series(data_yy, name='b-quark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tree(depth, Nmin):\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=depth, min_samples_leaf=Nmin)\n",
    "    tree_clf.fit(X, y)\n",
    "    graph = Source(tree.export_graphviz(tree_clf, out_file=None, filled = True))\n",
    "    display(SVG(graph.pipe(format='svg')))\n",
    "    return tree_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_plot = interactive(plot_tree, depth=(1,7), Nmin=(1,50))\n",
    "display(decision_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(max_depth=7, min_samples_leaf=25)\n",
    "tree_clf.fit(X, y)\n",
    "\n",
    "N, accuracy, fracWrong = evaluate(bquark)\n",
    "print(\"\\nDecision Tree BJET TAG:\")\n",
    "print(\"  First number in parenthesis is the estimate, second is the MC truth:\")\n",
    "print(\"  True-Negative (0,0)  = \", N[0][0])\n",
    "print(\"  False-Negative (0,1) = \", N[0][1])\n",
    "print(\"  False-Positive (1,0) = \", N[1][0])\n",
    "print(\"  True-Positive (1,1)  = \", N[1][1])\n",
    "print(\"    Fraction wrong            = ( (0,1) + (1,0) ) / sum = \", fracWrong)\n",
    "print(\"    Fraction right (accuracy) = ( (0,0) + (1,1) ) / sum = \", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment on example solution 3:\n",
    "\n",
    "This solution - a Decision Tree (from SKlearn) - is fundamentally different, as it is mostly automated, though it is still simple. However, \n",
    "\n",
    "improves the wrong fraction from <b>0.112</b> to <b>0.1078</b>, which is marginally better, though still not close to the Aleph NN solution of <b>0.0988</b>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
